{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLGyvAUvJE8gj2ZdGSVg1u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nutanpatil06/Fine-Tuning-LLM-with-LLaMA-Factory/blob/main/Fine_Tuning_LLM_with_LLaMA_Factory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2tkIijvrQHa"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "8AOQTZE_raIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/"
      ],
      "metadata": {
        "id": "f25AWasbrb54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "SfxJc3F1rek_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "msfgCx1srhJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "id": "S3yL2J8irhvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes>=0.39.0"
      ],
      "metadata": {
        "id": "Y5m7txSJrjc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/LLaMA-Factory/src/"
      ],
      "metadata": {
        "id": "tYxaMpPzrlOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GRADIO_SHARE\"] = \"1\""
      ],
      "metadata": {
        "id": "YfRZXoHNrm8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "budPtSadrsNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate transformers peft"
      ],
      "metadata": {
        "id": "U04ONsvkruba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/LLaMA-Factory/src/webui.py"
      ],
      "metadata": {
        "id": "sWYGv12erxaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from llamafactory.webui.interface import create_ui"
      ],
      "metadata": {
        "id": "zMN-zVRbr0tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ui = create_ui()"
      ],
      "metadata": {
        "id": "UPSiYoupr19j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ui.launch(share=True)"
      ],
      "metadata": {
        "id": "I9X1i2hbsBRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "#Base model (same as you used in training)\n",
        "base_model = \"google/gemma-1.1-2b-it\"\n",
        "\n",
        "#Path to your trained LoRA adapter\n",
        "lora_path = \"/content/LLaMA-Factory/saves/Gemma-1.1-2B-Instruct/lora/train_2026-01-06-09-51-36\""
      ],
      "metadata": {
        "id": "Kxori5W3sFgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "# Apply LoRA on top of base model\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    lora_path\n",
        ")"
      ],
      "metadata": {
        "id": "hQdxj-zjsHmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "\n",
        "# Enable evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# --- Inference prompt ---\n",
        "prompt = \"Can you tell what ls -l would display?\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate output\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "# Decode output\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "gZC2_agysOVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLI Based Methods"
      ],
      "metadata": {
        "id": "nYc7inyWsQ4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m llamafactory.cli train \\\n",
        "#   --model_name_or_path google/gemma-1.1-2b-it \\\n",
        "#   --template gemma \\\n",
        "#   --stage sft \\\n",
        "#   --finetuning_type lora \\\n",
        "#   --dataset yahma/alpaca-cleaned \\\n",
        "#   --output_dir output/my-gemma-qlora \\\n",
        "#   --cutoff_len 2048 \\\n",
        "#   --per_device_train_batch_size 1 \\\n",
        "#   --gradient_accumulation_steps 8 \\\n",
        "#   --num_train_epochs 1 \\\n",
        "#   --learning_rate 5e-5 \\\n",
        "#   --lora_rank 64 \\\n",
        "#   --lora_alpha 16 \\\n",
        "#   --lora_dropout 0.05 \\\n",
        "#   --quantization_bit 4 \\\n",
        "#   --fp16 True \\\n",
        "#   --gradient_checkpointing True \\\n",
        "#   --save_strategy epoch \\\n",
        "#   --save_steps 0 \\\n",
        "#   --save_total_limit 3 \\\n",
        "#   --logging_steps 10"
      ],
      "metadata": {
        "id": "zcd05LkIsPGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory"
      ],
      "metadata": {
        "id": "wPRaYwKIsWV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "8Xvt9R5BsXE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "N1F-sqllsZyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1"
      ],
      "metadata": {
        "id": "CR77hbZnscAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m llamafactory.cli train train_gemma_qlora.yaml"
      ],
      "metadata": {
        "id": "oCDH8bGDsdry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls output/my-gemma-qlora"
      ],
      "metadata": {
        "id": "w8kRP-fGsfyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -m llamafactory.cli train test.yaml"
      ],
      "metadata": {
        "id": "lG9s0WGlshTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m llamafactory.cli train \\\n",
        "#   --model_name_or_path google/gemma-1.1-2b-it \\\n",
        "#   --template gemma \\\n",
        "#   --finetuning_type lora \\\n",
        "#   --dataset yahma/alpaca-cleaned \\\n",
        "#   --output_dir output/debug \\\n",
        "#   --per_device_train_batch_size 1 \\\n",
        "#   --gradient_accumulation_steps 8 \\\n",
        "#   --num_train_epochs 1 \\\n",
        "#   --lora_rank 32 --lora_alpha 16 --lora_dropout 0.05 \\\n",
        "#   --save_strategy epoch --save_total_limit 1"
      ],
      "metadata": {
        "id": "yexzVFofsjOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !df -h\n",
        "# !touch output/debug/testfile && ls output/debug"
      ],
      "metadata": {
        "id": "tyhoHCNtslhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls output/test-lora"
      ],
      "metadata": {
        "id": "sS8ZixmdsmUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"output/test-lora\"))"
      ],
      "metadata": {
        "id": "k5nKEsdLspbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "base = \"google/gemma-1.1-2b-it\"\n",
        "adapter = \"/content/LLaMA-Factory/gemma_lora_sft_output\"   # your LoRA folder path\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base)\n",
        "model = AutoModelForCausalLM.from_pretrained(base, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "model = PeftModel.from_pretrained(model, adapter)\n",
        "\n",
        "prompt = \"Explain what is QLoRA\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "cxC5u8FksrMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yUvijpUPstDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m llamafactory.cli export train_gemma_qlora.yaml"
      ],
      "metadata": {
        "id": "HQ6KtXqBsuk7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}